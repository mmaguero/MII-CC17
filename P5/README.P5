#!/bin/bash

#Parametros
#Este script recibe 4 parametros: ./README.P5 64 10 N mcc4423998
#Maps nro.
MAPS=$1
#Tree nro.
TREES=$2
#Ejecución incial
INITIAL="$3"
#Ejecución incial
USR="$4"
echo "MAPS: $MAPS | TREES: $TREES | INITIAL: $INITIAL | USER: $USR"

#Copiar fichero JAR simplificado de mahout
cp /tmp/mahout-distribution-sige.jar .
JARFILE="/home/$USR/mahout-distribution-sige.jar"

DATASET="BNG_heart"
DATAOUT="/user/$USR"
DATAPATH="$DATAOUT/datasets"

#copiar dataset
if [ "$INITIAL" == "Y" ]; then
 hdfs dfs -mkdir $DATAPATH/
 hdfs dfs -mkdir $DATAPATH/$DATASET/
 hdfs dfs -cp /user/ahilario/datasets/BNG_heart/* $DATAPATH/$DATASET/
 hdfs dfs -ls $DATAPATH/$DATASET/
fi

#Valores

#Mahout
#Indicar el tamaño de los datos para cada partición
#Obtener el tamaño del fichero de entrenamiento
FILE_SIZE=( `hadoop fs -ls $DATAPATH/$DATASET/$DATASET-5-1tra.dat | awk '{print $5}'`)

#Calcular el tamaño de cada partición en función del número de Maps elegido
BYTES_BY_PARTITION=$((FILE_SIZE/$MAPS))

#Crear una nueva variable para el máximo valor del Split (minimo + 1)
MAX_BYTES_BY_PARTITION=$((BYTES_BY_PARTITION+1))

#Borrar outputs anteriores
if [ "$INITIAL" != "Y" ]; then
 hdfs dfs -rm output_RF*/*
 hdfs dfs -rmdir output_RF*
fi

#Generar los descriptores del conjunto de datos necesarios para la ejecución de los métodos (BNG_heart.header)
if [ "$INITIAL" == "Y" ]; then
 hadoop jar $JARFILE org.apache.mahout.classifier.df.tools.Describe \
 -p $DATAPATH/$DATASET/$DATASET-5-1tra.dat \
 -f $DATAOUT/$DATASET.info -d N C 3 N 2 C N C N 3 C L;
fi

#Ejecutar el clasificador con los parámetros asociados
hadoop jar $JARFILE \
 org.apache.mahout.classifier.df.mapreduce.BuildForest \
 -Dmapreduce.input.fileinputformat.split.minsize=$BYTES_BY_PARTITION \
 -Dmapreduce.input.fileinputformat.split.maxsize=$MAX_BYTES_BY_PARTITION \
 -o output_RF_${TREES}_${MAPS} \
 -d $DATAPATH/$DATASET/$DATASET-5-1tra.dat \
 -ds  \
 -sl 13 -p -t $TREES;
 
#Obtener la salida para el conjunto de test
hadoop jar /tmp/mahout-distribution-sige.jar \
 org.apache.mahout.classifier.df.mapreduce.TestForest \
 -i $DATAPATH/$DATASET/$DATASET-5-1tst.dat \
 -ds $DATASET.info \
 -m output_RF_${TREES}_${MAPS} \
 -a -mr -o output_RF_predict_out_${TREES}_${MAPS};

hadoop dfs -cat da_confusion_matrix.txt

#Obtener la salida para el conjunto de train
hadoop jar /tmp/mahout-distribution-sige.jar \
 org.apache.mahout.classifier.df.mapreduce.TestForest \
 -i $DATAPATH/$DATASET/$DATASET-5-1tra.dat \
 -ds $DATASET.info \
 -m output_RF_${TREES}_${MAPS} \
 -a -mr -o output_RF_train_out_${TREES}_${MAPS};

hadoop dfs -cat da_confusion_matrix.txt


